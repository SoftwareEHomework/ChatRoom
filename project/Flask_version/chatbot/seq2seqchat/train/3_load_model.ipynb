{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import jieba\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./word2id.json','r') as f:\n",
    "    word2id = json.load(f)\n",
    "with open('./id2word.json','r') as f:\n",
    "    id2word = json.load(f)\n",
    "id2word = {int(k):v for k,v in id2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, encoder_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder_units = encoder_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.encoder_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden) #全部隐藏状态，最后一个隐藏状态\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoder_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self,units):\n",
    "        super(Attention,self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self,query,values):\n",
    "        hidden_with_the_time_axis = tf.expand_dims(query,1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + \n",
    "                                 self.W2(hidden_with_the_time_axis)))\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(score,axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector,axis=1)\n",
    "        \n",
    "        return context_vector , attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,vocab_size,embedding_dim,decoder_units,batch_size):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoder_units = decoder_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.decoder_units,\n",
    "                                      return_sequences=True,\n",
    "                                      return_state=True,\n",
    "                                      recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = Attention(self.decoder_units)\n",
    "    \n",
    "    def call(self,x,hidden,encoder_output):\n",
    "        context_vector, attention_weights = self.attention(hidden,encoder_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector,1),x],axis=-1)\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        output = tf.reshape(output,(-1,output.shape[2]))\n",
    "        \n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x,state,attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 114969\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = 114969 //BATCH_SIZE\n",
    "embedding_dim = 100\n",
    "units = 512\n",
    "vocab_size = len(word2id)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_size,embedding_dim,units,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop()\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1a5ae8c1518>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    for ch in \"#@$%^&*():;：；{}[]'_<>-+/~0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\":\n",
    "        sentence = sentence.replace(ch,\"\")\n",
    "    sentence = jieba.cut(sentence)\n",
    "    sentence = [word2id.get(word,word2id[\"<pad>\"]) for word in sentence]\n",
    "    sentence = tf.keras.preprocessing.sequence.pad_sequences([sentence],value=word2id[\"<pad>\"],\n",
    "                                                             maxlen=20,padding='post')\n",
    "    sentence = tf.convert_to_tensor(sentence)\n",
    "    \n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1,units))] #现在只输入了一个所以batch_size是1\n",
    "    encoder_output, encoder_hidden = encoder(sentence,hidden)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_input = tf.expand_dims([word2id[\"<start>\"]],0)\n",
    "    \n",
    "    for t in range(20):\n",
    "        predictions , decoder_hidden , attention_weights = decoder(decoder_input,\n",
    "                                                                   decoder_hidden,\n",
    "                                                                   encoder_output)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        if id2word[predicted_id] == \"<end>\":\n",
    "            return result\n",
    "        result += id2word[predicted_id]\n",
    "        \n",
    "        decoder_input = tf.expand_dims([predicted_id],0)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 你说的什么哦？换种说法行不'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"百度一下\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
